---
title:  "A Journey to Fast Exploit Management in A/D"
description: "Fast is a tool developed to manage exploits and automate flag submission in A/D competitions."
categories: ['Blog']
tags: ['A/D', 'ECSC']
permalink: a-journey-to-easier-exploit-management-in-ad
read_time: 14
date: 2023-07-27 00:20 +0200
image:
  path: /assets/img/cards/tjctf-2023-web-challenges-writeup.png
---

If you have played jeopardy CTFs before, you already understand how flag submission works — exploit a service manually or using a script, and submit the flag to the platform. But when it comes to attack/defense, you will also have to:

- **Automate your exploits**: Automate everything! From user registration and login (if there is any), to sending the actual payload that gets you the flag. Everything has to be easily repeatable.
- **Automate your attacks**: Attack everyone! Run the same exploit at every other team to get their flags. Do the same for every exploit you write.
- **Automate flag submission**: Flags are updated on each tick and expire after one or more ticks. You must submit them frequently.

In A/D competitions, these are the things that you need to take care of besides finding vulnerabilities and patching, and each of these can give you a headache.

As an illustration of how our approach to solving these tasks evolved, I will share experiences from several A/D competitions in which I participated as a member of both the CyberHero team, and the extended Serbian national team for European Cyber Security Challenge (ECSC).

# Motivation — BSides Ljubljana A/D CTF Competition

This competition was hosted within [BSides Ljubljana cybersecurity event](https://0x7e7.bsidesljubljana.si/){:target="_blank"} and it took place on 16th of June 2023. I participated with two teammates from the extended national team, Žare and Prox.

My teammates did great, the competition was extremely fun and we managed to get third place. 🥉 Another team from CyberHero placed second, congrats! 🥈

![](/uploads/{{ page.permalink }}/bsides-scoreboard.png)

Although we achieved a good result, I strongly believe that we would perform even better if we could focus more on exploiting and patching, instead of writing and managing scripts.

![Footage of me restarting each script that hangs or crashes.](https://media.tenor.com/PdUQhVIQyCsAAAAd/mega64-hacked.gif)

☝️ Footage of me restarting each script that hangs or crashes.

## Identifying what went wrong

Throughout the competition, you will likely write lots of code that is far from perfect and is intended for single-use, with the sole criterion being: *it just needs to work*. At least that's what I say before showing my code from a CTF to someone.

With that being said, this is how our exploit scripts looked like:

```python
from submit import submit_all

def main():
    flags = []

    targets = [
        '10.0.0.2',
        # '10.0.0.4',
        '10.0.0.5',
        '10.0.0.6',
        '10.0.0.7',
        '10.0.0.8',
        '10.0.0.9',
        '10.0.0.10',
        '10.0.0.11'
    ]

    for target in targets:
        flag = requests.get(f"http://{target}:5555/search?admin=true;flag=destee").text.split("Flag: ")[1].strip()
        print("Found flag", target, flag)
        flags.append(flag)
    
    submit_all(flags)


if __name__ == "__main__":
    while True:
        try:
            main()
        except Exception:
            pass
        time.sleep(80)
```

Here are some problems with this code:

- **Lots of boilerplate code**: The only part that is actually about the exploit is inside the `for target in targets` loop. We had to copy-paste everything, and place our actual exploit code in between.
- **Attacking teams sequentially**: We ran exploits target by target, which is a pure waste of time. When a request timed out during an attack on some team, our script would wait instead of moving on to the other teams. Sometimes we would just remove the targets that caused timeouts or errors, like we did with `10.0.0.4` in the script above.
- **Time drift**: The tick length was 80 seconds, so we just called `time.sleep(80)` after the attacks complete to wait for the next run. Since our attacks took some amount of time to complete, the interval was not precisely 80 seconds, but rather 80 seconds plus the time taken by the attacks. This resulted in a time drift on some of our exploits, making them run too late and miss some flags.
- Other problems related to exception handling, efficiency, CPU and memory usage, etc.

Most of these problems can be mitigated in the code above, but that does not fix another problem that I did not mention, which is maybe the root of all the other problems:
- **The need to write any of this during the competition**.

## Developing a specialized tool

Attack/defense competitions carry a unique atmosphere. The urgency to start running exploits as soon as possible to get more attack points, patching your services quickly to not lose defense points, then out of nowhere you have to check that one service that nobody even touched but is down for some reason, and so on.

Writing reliable, efficient and reusable solutions in such an environment may not be a great idea. Since the next A/D competition was set for the 9th of July, less than a month away, I decided to start writing a flag acquisition and submission tool, later to become [Flag Acquisition and Submission Tool](https://github.com/dusanlazic/fast){:target="_blank"}, or Fast. As soon as I finished with the exams at my university, I started working on it.

At the very beginning, I was focused on solving the problems we encountered during BSides Ljubljana. These are some of the goals I had in mind: 
- Get rid of the boilerplate code and redefine a very simple exploit template.
- Use [threading](https://docs.python.org/3/library/threading.html){:target="_blank"} when running an exploit on multiple targets, instead of doing it sequentially.
- Make it easily configurable using a human-readable syntax (yaml with the help of [PyYAML](https://pyyaml.org/wiki/PyYAMLDocumentation){:target="_blank"}).
- Keep it simple and "competition-environment-friendly".

After less than a week of work, the result was the following. Fast was meant to be used like this:

1. Install Fast using pip.

```
git clone --depth 1 https://github.com/dusanlazic/fast.git
pip install -e fast/
```
This way, you will be able to run Fast from anywhere.

{:start="2"}
2. Write exploits this way:

```python
# alpha.py
import requests

def exploit(target: str) -> str:
  return requests.get(f"http://{target}:5555/search?admin=true;flag=destee").text
```

```python
# beta.py
import requests

def exploit(target: str) -> str:
  return requests.get(f"http://{target}:3334/files/..%2Fflag.txt").text
```

Your exploit scrips should have a function named `exploit` that takes target's host as a parameter, and returns a string that contains the flag. That's about it. Flag does not have to be clean, Fast will match it.

{:start="3"}
3. Write a submitter (`submitter.py`) that follows this template:

```python
from typing import List, Tuple

def submit(flags: List[str]) -> Tuple[List[str], List[str]]:
  # Submit flags and determine which were accepted and rejected.
  return accepted_flags, rejected_flags
```

{:start="4"}
4. Create `fast.yaml` and configure the game, flag submission, and of course, exploits and targets:

```yaml
game:
  tick_duration: 80
  flag_format: CTF\{[a-f0-9]{40}\}
  team_ip: 10.0.0.3

submitter:
  tick_start_delay: 30

exploits:
  - name: alpha
    targets:
    - 10.0.0.2-11
  
  - name: beta
    targets:
    - 10.0.0.2-7
    - 10.0.0.9
  
  - name: gamma
    targets:
    - 10.0.0.2-10
```

{:start="5"}
5. Place all those files in the same directory, and run Fast:

```
fast
```

After starting Fast, it will immediately start running exploits and it will automatically submit flags on a schedule.

Running exploits and flag submission were scheduled using [APScheduler](https://apscheduler.readthedocs.io){:target="_blank"}, so no more time drifts. Each exploit gets its own Python interpreter (to bypass GIL), and each target gets its own Python thread. Since exploits are primarily I/O bound tasks, [threading](https://docs.python.org/3/library/threading.html){:target="_blank"} greatly improves the performance when attacking multiple targets.

In this version, all retrieved flags were stored inside a local SQLite database, and they were submitted periodically according to the user's configuration. Every player had to run Fast on their own machine in order to run exploits and submit retrieved flags.

There were also some additional exploit settings that could be set in yaml:
- `cmd: ./delta [ip]` — Run a shell command instead of a Python module. This is useful for running non-python exploits.
- `timeout: 20` — Set a timeout to skip waiting for a hanging exploit.
- `delay: 5` — Set a delay to mitigate spikes in CPU/bandwidth usage when a tick starts.
- `env` — Set environment variables.

The first version was developed a bit too fast, but the tool was not meant to be perfect (yet). Battle-testing and improving it after each A/D was a part of the plan. 

Move fast and break things, I guess.

# First Battle-Test — Compete with Team Europe A/D

The next A/D competition we participated in was "Compete with Team Europe". In this event, 20 national teams from Europe had an opportunity to compete against Team Europe, which is set to represent the continent in the upcoming International Cybersecurity Challenge (ICC).

We gathered in person in Belgrade, at the premises of RATEL and National CERT of the Republic of Serbia, to participate in this event together.

![](/uploads/{{ page.permalink }}/ratel-2.jpeg)

During this competition, we battle-tested Fast for the first time. The results? 

Promising. 

But not quite there yet.

We placed 13th out of 21 teams. Our total offsense points were 976.28, which is lower than we hoped for.

![](/uploads/{{ page.permalink }}/cwte-scoreboard.png)

## What went wrong this time?

During the competition, I was observing and noting down how the tool was behaving, how it was complying with the game and the overall competition environment.

![](https://media.tenor.com/Nxo9h7sUnRIAAAAC/spongebob-patrick.gif)

Here are some observations and how they shaped the next version of Fast.

#### 1. Single team IP

We had three machines with different IPs instead of one, while `team_ip` setting was accepting only a single host (a host to skip when attacking). I fixed it in later versions to support a list of hosts.

```yaml
game:
  tick_duration: 120
  flag_format: ICC_\w{32}
  team_ip: [10.20.22.4, 10.20.22.5, 10.20.22.6]
```

#### 2. A false assumption about one flag per target

A teammate approached me with a brand new exploit. He exploited a vulnerability, accessed the related flag store and found multiple flags, all in the same response. Many of them were valid and non-expired. Fast expected (and extracted) only one flag, or the first match. Yikes!

To support matching multiple flags I had to modify the tool on the spot.

#### 3. Looks like a flag? Good enough.

> "My exploit just returned 87 duplicate flags and 3 new flags from this target? Ide gas. 👍"

It is always easier to just return all the flags, even if most of them are duplicates or expired, instead of filtering them. I did not prepare my tool for that. Flags were stored inside a local SQLite database that allowed storing duplicates. The only filter was applied during flag submission, which was submitting the flags marked as `queued`. Still, that was TONS of old, duplicate and some new flags. It would take the flag checking service up to 30 seconds to process all of them. Sorry for spamming, competition organizers!

We agreed that it may be better to have a centralized database for storing and checking flags, so no one ever submits a duplicate. A centralized flag store is also great for getting useful statistics, which we were lacking.

#### 4. Late problem discovery

Like I mentioned, we were lacking some statistics and alerts. It took me a while to realize that one of my exploits was retrieving and queuing the same set of flags each tick. The tool had no way to notify us that 100% of the flags retrieved by that exploit were duplicates. 

Instead of looking at the logs, we needed something like a "dashboard" with some fancy real-time data. Centralizing the database would certainly help with that.

#### 5. Flags were queued late

Flags were queued when the exploit is done with all targets, which was too optimistic. The idea was to take advantage of bulk inserts since they are more efficient, but unfortunatelly there were instances of timeouts at some targets, delaying flag queuing. The tool was changed to push new flags to the queue instantly.

#### 6. Configuration hell

Configuring Fast on multiple player machines did not go that well in the competition environment. Everyone had to manually configure fast.yaml the same way, keep an up-to-date copy of submitter.py, and so on. At the end, it was easier to just run all the exploits on my machine. 

This was another problem that could be solved by separating the tool into a client (exploit runner) and a server (flag submitter).

## Separating submitter from runners

The next A/D competition was Enowars7 on 22th of July. It was in less than two weeks, so my goals at the time probably were:
- Centralize flag submission
- Build a simple dashboard
- Focus on fault-tolerance

#### Misconfiguration tolerance

If the program crashes due to misconfiguration, I do not want to run around and point at what should be fixed. [jsonschema](https://python-jsonschema.readthedocs.io){:target="_blank"} will do it instead! Before running the server or a client, configuration will be checked for errors, and it will tell you what to fix.

The **exploits** section of fast.yaml is meant to be modified without restarting Fast. If you mess up the config, Fast will not crash, but it will reuse the previous working configuration until you sort it out.

#### Client configuration

Since the client and the server are now separated, two configuration files are used instead of one. For the client it is fast.yaml with its two sections: `connect` and `exploits`.
```yaml
connect:
  host: 192.168.1.49
  port: 2023
  player: s4ndu

exploits:
# same as before
```

Running the client is the same as before, except that now you do not need to have submitter.py on player's machine. After running `fast`, the client will configure itself and wait for the next tick to begin.

#### Server configuration

In this version, server holds the configuration of the game and the submitter in `server.yaml`.

```yaml
game:
  tick_duration: 120
  flag_format: FAST\{[a-f0-9]{10}\}
  team_ip: 172.20.0.5

submitter:
  delay: 10

server:
  host: 0.0.0.0
  port: 2023
```

Starting the server is done by running the command: `server`

It runs a [Flask](https://flask.palletsprojects.com){:target="_blank"} application with [SocketIO](https://flask-socketio.readthedocs.io){:target="_blank"} using a Flask development server. Usage of a development server was not ideal, so it was replaced after this version.

#### HTTP Basic authentication

If your server is publicly accessible, setting a password is a good idea. I added the support for HTTP Basic authentication using [Flask-HTTPAuth](https://flask-httpauth.readthedocs.io){:target="_blank"}.

```yaml
# server.yaml
server:
  host: 0.0.0.0
  port: 2023
  password: letmein
```

```yaml
# fast.yaml
connect:
  host: 192.168.1.49
  port: 2023
  player: s4ndu
  password: letmein
```

#### Storing flags

For better concurrency and better performance in write-heavy environments, I switched from SQLite to Postgres. Flag model was also redefined (included exploit name, tick number, target, etc.), giving us much more options for detailed analytics.

#### Flag-out detection

Instead of skipping attacks on our own hosts, Fast is modified to run exploits anyway. The flags from own services are discarded, and we are notified about our vulnerable services that require patching.

#### Monitoring dashboard

It is far from perfect and is written in vanilla JavaScript, which may be correlated in some way. It displays real-time data that is delivered through websockets.

![](/uploads/{{ page.permalink }}/dashboard.png)

#### Fault tolerance

Another ongoing objective is to make everything as much fault-tolerant as possible. Someone trips over a cable and shuts everything down? No problem, flags are safe and will be submitted on time, somehow.

# Second Battle-Test — ENOWARS7

